# This pipeline is designed to process Perun IdM audit log files.
# It uses a combination of grok, kv, dissect, and ruby filters to parse
# the log data into a structured format for further analysis

input {
  file {
    # Update to your actual Perun audit log path/pattern
    path => "/var/log/idm/perun_auditlog.log"
    start_position => "beginning"
    # The sincedb_path is set to /dev/null for testing. For production,
    # consider setting a relative path, e.g., "./.sincedb_perun-audit".
    sincedb_path => "/dev/null"
  }
}

filter {
  # Extract syslog header and capture the JSON payload
  grok {
    match => {
      "message" => [
        # Example line:
        # Apr 17 11:15:34 host perun_audit[53417]: {"id":..., "event":{...}, "actor":"...", "createdAt":"..."}
        "^%{SYSLOGTIMESTAMP:syslog_ts} %{HOSTNAME:syslog_host} perun_audit\[%{NUMBER:syslog_pid}\]: %{GREEDYDATA:perun_json}$"
      ]
    }
    tag_on_failure => ["_grok_syslog_failure"]
  }

  # Parse the JSON payload into a Perun event object
  json {
    source => "perun_json"
    target => "perun"
    tag_on_failure => ["_jsonparsefailure_perun"]
  }

  # Derive timestamp (prefer Perun's createdAt, fallback to syslog)
  # Perun createdAt examples: "2025-04-17 12:27:23.239712" (microseconds) or without micros.
  ruby {
    code => '
      created_at = event.get("[perun][createdAt]")
      if created_at
        begin
          parsed_time = DateTime.parse(created_at).to_time.utc
          event.set("@timestamp", LogStash::Timestamp.new(parsed_time))
        rescue
          event.tag("_date_perun_createdAt_failure")
        end
      else
        event.tag("_date_perun_createdAt_failure")
      end
    '
  }

  # Fallback to syslog timestamp if Perun createdAt missing/invalid
  if "_date_perun_createdAt_failure" in [tags] or ![perun][createdAt] {
    date {
      match => [ "syslog_ts", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
      target => "@timestamp"
      # syslog date has no year; Logstash assumes current year.
      tag_on_failure => ["_date_syslog_ts_failure"]
    }
  }

  # 4) Derive fields from the Perun JSON
  # Short action name from fully-qualified class in event.name
  mutate {
    copy => { "[perun][event][name]" => "perun_event_name" }
  }
  # e.g., cz.metacentrum.perun.audit.events.UserManagerEvents.UserExtSourceUpdated -> UserExtSourceUpdated
  ruby {
    code => '
      full = event.get("perun_event_name")
      if full.is_a?(String)
        event.set("perun_action", full.split(".").last)
      end
    '
  }

  # Derive Subject from the Perun actor
  mutate {
    add_field => {
      "subject" => "%{[perun][actor]}"
    }
  }

  # Derive Outcome heuristically: assume "success" unless message indicates failure
  mutate { add_field => { "outcome" => "success" } }
  if [perun][event][message] and [perun][event][message] =~ /(fail(ed)?|error|denied)/ {
    mutate { replace => { "outcome" => "failure" } }
  }

  # Create a structured 'accounting' field.
  ruby {
    code => '
      acct = {
        "timestamp" => event.get("[perun][createdAt]") || event.get("syslog_ts"),
        "subject" => event.get("subject"),
        "operation" => event.get("perun_action") || event.get("perun_event_name"),
        "outcome" => event.get("outcome"),
        "source" => "perun",
        # TODO Sample Perun audit lines do not include client IP
        # TODO "connection_protocol" => "perun?"
      }
      # Optional enrichers when available
      ["object","origin_system","destination_system"].each do |k|
        v = event.get(k)
        acct[k] = v if v
      end
      event.set("accounting", acct)
    '
  }

  # Keep only the accounting object
  ruby {
    code => '
      if event.get("accounting")
        accounting_data = event.get("accounting")
        event.to_hash.keys.each { |k| event.remove(k) unless k.start_with?("@") }
        accounting_data.each { |k,v| event.set(k, v) }
      end
    '
  }
}

output {
  # Example output to console for verification
  # stdout {
  #   codec => rubydebug
  # }

  # Uncomment this block to output to an Elasticsearch cluster.
  # Replace the 'hosts' and 'index' settings as needed for your environment.
  elasticsearch {
    hosts => ["http://localhost:9200"] # Change this to your Elasticsearch host and port
    index => "perun-audit-%{+YYYY.MM.dd}" # The index name for your data
    # You can also add authentication if your Elasticsearch requires it
    # user => "elastic"
    # password => "changeme"
